{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e206b29-c100-4edf-b5cb-3deff20c6f08",
   "metadata": {},
   "source": [
    "# Fine-tuning LLM for Wordpress Posts\n",
    "\n",
    "This document contains the code to fine-tune Gemma with Wordpress Posts for conversation as a chatbot assistant.\n",
    "\n",
    "**Database: Weaviate**\n",
    "\n",
    "**Collection: Post**\n",
    "\n",
    "**Schema**\n",
    "\n",
    "```\n",
    "class Post(WeaviateCollection):\n",
    "    id:str\n",
    "    postId:str\n",
    "    postTitle:str\n",
    "    postExcerpt:str\n",
    "    postContent:str\n",
    "    postDate:datetime\n",
    "    postAuthor:str\n",
    "    postCategories:typing.Optional[str]\n",
    "    postTags:typing.Optional[str]\n",
    "    postUrl:typing.Optional[str]\n",
    "    postSequence:typing.Optional[int]=1\n",
    "    isDeleted:typing.Optional[bool]=False\n",
    "\n",
    "    def get_embedding(self):\n",
    "        return [0.12345] * 1536\n",
    "```\n",
    "\n",
    "## 1.0 Steps to create embeddings\n",
    "\n",
    "We will generate embeddings externally using Gemma's tokenizer and store it in weaviate. This will enable not only to perform semantic search in the databaset, it will also keep the vectors prepared and stored reducing the time to generate vectors each time a request is made.\n",
    "\n",
    "1. The following parameters will be included in the embedding:\n",
    "\n",
    "    - postContent: Main content of the post\n",
    "    - postTitle: Title of the post\n",
    "    - postAuthor: Author of the post\n",
    "    - postDate: Publish date of the post\n",
    "    - postCategories: Categories that the post belongs to\n",
    "    - postTags: Tags related to the post\n",
    "\n",
    "    \n",
    "\n",
    "2. The following type of embeddings will be available for the system.\n",
    "\n",
    "    - **Fine-tuning:** When fine-tuning the model, the embedding will contain post contents with some metadata. The text needs to be conversational.\n",
    "    - **Query:** When querying the LLM, the text that will be converted to embedding will be different from the ones that are used for fine-tuning. The text will contain instructions, user prompt, and a context. The combined text will form the embedding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140283fd-f7a2-47f7-99f8-0c5287a88633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "import weaviate\n",
    "import typing\n",
    "from weaviate.connect import ConnectionParams\n",
    "from weaviate.classes.init import AdditionalConfig, Timeout, Auth\n",
    "from weaviate.classes.config import DataType, Configure, Property\n",
    "from weaviate.classes.query import Filter\n",
    "from abc import ABC, abstractmethod\n",
    "import strawberry\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import json\n",
    "import httpx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f2a66c-0397-46c9-a9fb-ea86629c5582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add HF_HOME and HF_TOKEN to env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03209b7c-6100-4d33-9452-aca3b98581b6",
   "metadata": {},
   "source": [
    "## 2.0 Weaviate configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7612ef-2ea1-48f0-9e85-b2a00a47adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_host = \"localhost\"\n",
    "weaviate_port = 50050\n",
    "weaviate_grpc_port = 50051\n",
    "weaviate_scheme = \"http\"\n",
    "weaviate_user = \"admin@vip3rtech6069.com\"\n",
    "weaviate_key = \"admin123\"\n",
    "\n",
    "weaviate_connection_params = ConnectionParams.from_params(\n",
    "    http_host=weaviate_host,\n",
    "    http_port=weaviate_port,\n",
    "    http_secure=weaviate_scheme == \"https\",\n",
    "    grpc_host=weaviate_host,\n",
    "    grpc_port=weaviate_grpc_port,\n",
    "    grpc_secure=weaviate_scheme == \"https\"\n",
    ")\n",
    "weaviate_auth_secret = Auth.api_key(weaviate_key)\n",
    "weaviate_additional_config = AdditionalConfig(\n",
    "    timeout=Timeout(init=30, query=60, insert=120),  # Values in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1086cb5-dbb6-4f39-b116-ebbdb3a6ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_client = weaviate.WeaviateAsyncClient(\n",
    "    connection_params=weaviate_connection_params,\n",
    "    auth_client_secret=weaviate_auth_secret,\n",
    "    additional_config=weaviate_additional_config,\n",
    "    skip_init_checks=True\n",
    ")\n",
    "\n",
    "await async_client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0e6648-61b7-438b-9916-5f9eeea6b3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await async_client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f74e6c-0ec8-4f9e-8ffa-09803c11951d",
   "metadata": {},
   "source": [
    "## 3.0 Wordpress configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9554b43e-4cd0-4b27-8bf5-abe822af5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeaviateCollection(ABC):\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def from_dict(data:dict)->typing.Type[typing.Any]:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def to_dict(data:typing.Any)->dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_embedding(self)->typing.List[float]:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_field_mapping()->dict:\n",
    "        pass\n",
    "    \n",
    "\n",
    "@strawberry.type\n",
    "class Post(WeaviateCollection):\n",
    "    id:str\n",
    "    postId:str\n",
    "    postTitle:str\n",
    "    postExcerpt:str\n",
    "    postContent:str\n",
    "    postDate:datetime\n",
    "    postAuthor:str\n",
    "    postCategories:typing.Optional[str]\n",
    "    postTags:typing.Optional[str]\n",
    "    postUrl:typing.Optional[str]\n",
    "    postSequence:typing.Optional[int]=1\n",
    "    isDeleted:typing.Optional[bool]=False\n",
    "\n",
    "    @staticmethod\n",
    "    def to_dict(post):\n",
    "        return {\n",
    "            \"id\": post.id,\n",
    "            \"postId\": post.postId,\n",
    "            \"postTitle\": post.postTitle,\n",
    "            \"postExcerpt\": post.postExcerpt,\n",
    "            \"postContent\": post.postContent,\n",
    "            \"postDate\": post.postDate,\n",
    "            \"postAuthor\": post.postAuthor,\n",
    "            \"postCategories\": post.postCategories,\n",
    "            \"postTags\": post.postTags,\n",
    "            \"postUrl\": post.postUrl,\n",
    "            \"postSequence\": post.postSequence,\n",
    "            \"isDeleted\": post.isDeleted\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dict(data:dict):\n",
    "        if not data or type(data) is not dict:\n",
    "            return None\n",
    "\n",
    "        return Post(\n",
    "            id=data.get(\"id\", \"\"),\n",
    "            postId=data.get(\"postId\", \"\"),\n",
    "            postTitle=data.get(\"postTitle\", \"\"),\n",
    "            postExcerpt=data.get(\"postExcerpt\", \"\"),\n",
    "            postContent=data.get(\"postContent\", \"\"),\n",
    "            postDate=data.get(\"postDate\", \"\"),\n",
    "            postAuthor=data.get(\"postAuthor\", \"\"),\n",
    "            postCategories=data.get(\"postCategories\", \"\"),\n",
    "            postTags=data.get(\"postTags\", \"\"),\n",
    "            postUrl=data.get(\"postUrl\", \"\"),\n",
    "            postSequence=data.get(\"postSequence\", 1),\n",
    "            isDeleted=data.get(\"isDeleted\", False)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_field_mapping()->dict:\n",
    "        return {\n",
    "            'postId': 'id',\n",
    "            'postTitle': 'title',\n",
    "            'postExcerpt': 'excerpt',\n",
    "            'postContent': 'content',\n",
    "            'postDate': 'date_gmt',\n",
    "            'postAuthor': 'author',\n",
    "            'postCategories': 'categories',\n",
    "            'postTags': 'tags',\n",
    "            'postUrl': 'link',\n",
    "            'postSequence': 'sequence',\n",
    "            'isDeleted': 'isDeleted'\n",
    "        }\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aec29-cd61-4ea2-874c-4c8140d018f0",
   "metadata": {},
   "source": [
    "## 4.0 LLM Configuration\n",
    "\n",
    "**Model: Gemma-2-2b**\n",
    "\n",
    "**Source: [https://huggingface.co/google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b)**\n",
    "\n",
    "**Source: [Fine-tuning Llama2](https://github.dev/krishnaik06/Finetuning-LLM/blob/main/Fine_tune_Llama_2.ipynb)**\n",
    "\n",
    "**Quantization: 4bit**\n",
    "\n",
    "**Tokenizer: Gemma-2-2b**\n",
    "\n",
    "**Technique: [Lora](https://huggingface.co/docs/peft/package_reference/lora)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622b24c-f1ef-4a7c-b39b-488c822a4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate bitsandbytes peft scikit-learn scipy trl transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4ddf3-8b4b-4044-9240-20a26ef4619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuning use pytorch with cuda. For backend application, use the one without cuda to reduce unnceccessary package size\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98c1057a-98a5-4cc5-8271-93540fa281c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f362e379-4e36-49d1-872f-feb8e5d41251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\accelerate\\utils\\other.py:220: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  np.core.multiarray._reconstruct,\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0797a61-35da-4519-b03e-8d5535cd3086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory Allocated: 0 bytes\n",
      "Memory Cached: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_details():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "        print(f\"Memory Cached: {torch.cuda.memory_reserved()} bytes\")\n",
    "\n",
    "get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6519ff6d-e811-4c11-9e7c-d2dd3e2d7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Clearing GPU memory...\")\n",
    "        torch.cuda.empty_cache()  # Clears the GPU cache\n",
    "        torch.cuda.reset_peak_memory_stats()  # Resets memory stats tracking\n",
    "        torch.cuda.synchronize()  # Ensures all streams are synced (optional)\n",
    "        print(\"GPU memory cleared.\")\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU memory to clear.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83940a04-e6af-4f2a-9ca3-14fb4d764c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in E:\\Accessories\\Research\\NeuralNetwork\\huggingface\\hub\\models--google--gemma-2b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards:   0%|                                                                        | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Define the model and tokenizer\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Create a sample input\n",
    "text = \"The GPU is being tested with transformers!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform a forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Verify that the tensors are on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    is_on_gpu = all(tensor.device.type == 'cuda' for tensor in outputs.values())\n",
    "    if is_on_gpu:\n",
    "        print(\"Success: Transformers library is using the GPU.\")\n",
    "    else:\n",
    "        print(\"Warning: Transformers library is not using the GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "# Optional: Print GPU details\n",
    "get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49550409-5ba8-478a-924a-0fe68591ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121cb15-975a-4b3a-a0b4-6c5fc4d3c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"google/gemma-2b-it\" #\"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = f\"{model_name}-chat-finetune\" #\"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = 'cuda' # {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b25eca-3c69-4a52-8084-3864b644d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83c2d7-efe3-4347-8786-63a8b3fd6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        token=hf_token,\n",
    "    )\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d183fa6-eb47-4234-a8cb-e68bc80df1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemma tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691316f9-7673-4d55-a774-97806d06be2c",
   "metadata": {},
   "source": [
    "## 5.0 Predict output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459e3e5-558a-49ab-b465-a6d1bb1464e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output using pipeline and predefined prompts by HuggingFace\n",
    "\n",
    "text = \"What is the origin of 'Hello World!'\"\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "print(type(encoded_text))\n",
    "print(tokenizer.encode(text, return_tensors='pt').to(\"cuda\"))\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(text)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31251c32-26ba-41da-b92b-90e8520fed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output by constructing prompt from raw text query\n",
    "text = \"What is the origin of 'Hello World!'\"\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": text },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1af55f-0588-41e4-9204-c4f9c21ee7b0",
   "metadata": {},
   "source": [
    "## 6.0 Prepare dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2659c-26f2-44d2-abc6-e3e7ba985dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
