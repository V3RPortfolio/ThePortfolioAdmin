[{"text": "Post Title: Containerized Django Development II: CORS and Views\nPost Author: Zohair Mehtab\nPost Date: 2024-11-01 20:55:54+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django,docker\nPost Content: . As we peer into the network tab of our developer tools, we notice something interesting. The initial request, devoid of any identifying cookies or authorization headers, sails through unimpeded. Django, it seems, is welcoming to first-time visitors. However, a closer look at the response reveals a Set-Cookie header, subtly instructing the browser to store a csrftoken. This is our first encounter with Django\u2019s security measures, quietly setting the stage for future interactions.\u00a0We can notice a couple of things in the HTTP response returned by our Django application.1. No authorization headers:\u00a0The request does not contain any cookie or authorization headers.2. Set-Cookie Header: On the other hand, when you observe the HTTP response, you can see the header called \u2018Set-Cookie\u2019. This is our Django application instructing the browser to set the cookies returned as the value.\u00a03. Cross-Origin-Opener-Policy (COOP) header: When we open a webpage in our browser, it loads within a specific browsing context\u2014a container, like a browser window, where the page operates. If we embed an iframe on that page, the iframe has its own browsing context since it can load content from external sites. Without a restrictive header, all web pages and embedded elements like iframes could potentially share a global browsing context. This shared access could allow an attacker to gain unauthorized entry to another page\u2019s browsing context within the same window and potentially access sensitive information. COOP\u00a0enables the browser to control and isolate each page\u2019s browsing context. For instance, setting the header to \u2018same-origin\u2019 ensures that only pages from the same origin (e.g., localhost) can access the browsing context of this page. For more in-depth details, I\u2019ve included additional links in the references section. This header is closely related to CORS (Cross-Origin Resource Sharing) since it allows us to define how external applications with different origins can interact with our webpage.\u00a0 \n \nWe can observe that our browser has stored the \u2018csrftoken\u2019 in cookies, as directed by our backend application.\u00a0Interestingly, we also notice that our initial HTTP GET request succeeded even though the browser didn\u2019t send a CSRF token to the backend.Intrigued, we reload the page, simulating a returning visitor. This time, the request proudly displays the csrftoken within its Cookie header, like a badge of authenticity. Django, recognizing the token, grants access without hesitation.But the plot thickens! As we delve deeper, we observe a curious discrepancy. The csrftoken nestled within the browser\u2019s cookie jar differs from the {% csrf_token %} embedded in our index.html form. This unexpected twist raises questions about the purpose of this seemingly duplicate token and its role in the grand scheme of CSRF protection.\u00a0 \n \nWhile the cookie-based token acts as a long-term pass, granting access for the duration of our session, its form-bound counterpart plays a more dynamic role. This token, regenerated with every page load, ensures that each form submission carries a unique and unpredictable identifier."}, {"text": "Post Title: Containerized Django Development II: CORS and Views\nPost Author: Zohair Mehtab\nPost Date: 2024-11-01 20:55:54+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django,docker\nPost Content:  \nContainerized Django Development: Part 2 \nExperimenting CORS with Django Views \n1.0 Overview \nIn our last adventure, we embarked on a journey to containerize our Django application with Docker and GraphQL. We even touched upon the guardians of our API: CORS and CSRF. But that was just a glimpse into their power! If you have not read the previous article, I will highly recommend it reading here.\nThis time, we\u2019re diving deep into the trenches of CORS implementation. We\u2019ll explore the real-world challenges I faced when deploying my application to production \u2013 those \u201coh no!\u201d moments when things didn\u2019t go quite as planned (because, let\u2019s be honest, who hasn\u2019t been there?).\nAnd fear not, fellow developers! We won\u2019t leave you stranded in a sea of confusion. I\u2019ll guide you through the proper implementation of CORS and CSRF in Django, demystifying those tricky configurations that often send newbies running for the (sometimes dangerous) comfort of @csrf_exempt.\nYou can find the complete source code with a step-by-step setup guide here.\u00a0But in this article, we\u2019re all about the \u201cwhy\u201d and the \u201chow,\u201d so get ready for a deep dive into the theory behind these security essentials! \n2.0 Taking Down the Walls (Temporarily!) \nBefore we construct our fortress of security, let\u2019s first understand why these safeguards are so important. To do that, we\u2019ll temporarily disable our defenses and see what happens when chaos reigns free!\u00a0Our Django application relies on two key files for CORS and CSRF protection: settings.py and the .env file. Think of the .env file as our security control panel, where we define who\u2019s allowed to knock on our API\u2019s door. It contains two crucial variables:1.\u00a0DJANGO_ADMIN_SERVICE_ALLOWED_HOSTS: This is our guest list, specifying which domains have access to our API endpoints. Initially, it\u2019s set to an asterisk (*), meaning everyone is invited to the party! (Not ideal for security, but great for our experiment).2.\u00a0DJANGO_ADMIN_CSRF_TRUSTED_ORIGIN: This variable determines who we trust with our secret CSRF tokens. It\u2019s initially empty, meaning we\u2019re not handing out any secret handshakes just yet.The settings.py file, on the other hand, is where the actual CORS and CSRF magic happens. It contains the code that enforces these security measures based on the rules we define in our .env file. \nALLOWED_HOSTS = os.getenv('ALLOWED_HOSTS', '').split(',')\nCORS_ALLOW_CREDENTIALS = True\n \nThe code snippet provided above contains the configuration related to CORS and CSRF protection.\u00a0Now, let\u2019s say we want to tighten security and only allow requests from localhost. We can update our .env file like this:DJANGO_ADMIN_SERVICE_ALLOWED_HOSTS=\u201dlocalhost\u201dDJANGO_ADMIN_CSRF_TRUSTED_ORIGIN=\u201dhttp://localhost:4200\u2033This tells our Django app: \u201cHey, only allow requests from applications running on localhost. And while you\u2019re at it, only trust http://localhost:4200 with CSRF tokens.\u201dOf course, we can add multiple comma-separated domains and origins if we have a more extensive guest list. But for our experiment, we\u2019ll keep it simple.Now, for the fun part! To truly appreciate the importance of CORS and CSRF, we\u2019ll temporarily comment out all the related configurations in settings.py."}, {"text": "Post Title: Unit Testing with .NET II: Refactoring the code\nPost Author: Zohair Mehtab\nPost Date: 2024-11-14 07:56:28+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,c#,mspec,xunit\nPost Content:  \nUnit Testing with .Net II \nRefactoring the code \n1.0 Overview \nIn the previous article, we discussed what unit tests are, the purpose of unit tests, and the basic concepts of MSpec and xUnit tests. In this article, we will go through a simple .Net console application that we have created to demonstrate the implementation of MSpec and xUnit test cases. We will discuss how we usually write our code and what we should do to write code that is adaptable to unit testing. The code is available on GitHub. Please click here to view the application code. \n2.0 Overview of the application \nOur application is a simple web scraping app that fetches the HTML content from a Wikipedia URL and outputs the content in the console. Our codebase contains two types of implementation of the web scraper. One is implemented in a simple way which we follow when writing code without considering unit tests. This code can be found in the CodeNotAdapatableToUnitTest folder of the GitHub link. The other implementation that can be found in the Code folder contains the code structure that we follow when are considering making our codebase adaptable to unit tests. The folder Domain contains the class which is shared between the two types of implementation. We can find the MSpec tests in MSpec folder and the xUnit tests in XUnit folder. The file Program.cs is the entry point to our application. It contains the required dependencies and manages the dependency injection in our application. It also instantiates the base WebScraper class and displays the final output in the console.To keep the demonstration simple for this tutorial, I have not implemented unit tests for all the classes. I have just written the unit tests for the WebScraper.cs class now. However, in a later tutorial, I am planning to discuss the implementation of unit tests for other classes of this project as well.You will notice that I have used threading-related Tasks to synchronize the async methods. I followed this approach so that I can demonstrate how we write unit tests for synchronous methods first. Once we acquire a strong grasp on unit tests, we can modify our code to write unit tests for async methods as well. Whenever you see the following type of code: \nvar task = Task.Run(() => {\n try\n {\n return client.GetAsync(url);\n }\n catch (Exception e)\n {\n Console.WriteLine($\"Http Get request error for url {url}.\");\n Console.WriteLine(e.Message);\n Console.WriteLine(e.StackTrace);\n return null;\n }\n});\ntask.Wait();\nvar response = task.Result;\n \nignore the complication and assume that we are just synchronously fetching responses from an asynchronous method. \n3.0 The Naive Way of Coding \nFilename: WebScraper.csAs discussed earlier, the WebScraper file makes a GET request to a specific URL and returns the result. To make the request, we use the HttpClientFactory object provided by C#.First, we declare our constructor and inject the HttpClientFactory object."}, {"text": "Post Title: Traversing a spiral matrix\nPost Author: Zohair Mehtab\nPost Date: 2024-11-17 16:15:52+00:00\nPost Categories: Arrays,Blogs,Data Structure\nPost Tags: array,competitive programming,data structure,matrix,spiral,traversal\nPost Content: . \ndef can_move_up(visited, current_row, current_col, total_rows, total_cols):\n up_index = to_index(current_row - 1, current_col, total_cols)\n return current_row - 1 >= 0 and not visited[up_index]\n \nRequired Variables\nTo effectively traverse the matrix in a spiral order, we need to keep track of several key pieces of information: \ndef spiralOrder(matrix):\n \"\"\"\n This function traverses a matrix in spiral order and returns the list of elements.\n @param matrix List[List[int]]: The matrix items\n \"\"\"\n total_rows = len(matrix)\n total_cols = len(matrix[0])\n # Current index being traversed\n current_row = 0\n current_col = 0\n \n # Start by moving right. Available directions: l (left), r (right), u (up), d (down)\n current_direction = 'r'\n # List of cells visited\n visited = [False] * (total_rows * total_cols)\n # keep track if we can move in the direction - Initially start by moving right\n move_right = True\n move_left = False\n move_down = False\n move_up = False\n \nIn this code:\n1. total_rows and total_cols store the dimensions of the matrix.\n2. current_row and current_col maintain the current position during traversal.\n3. current_direction indicates the current direction of movement (\u2018r\u2019 for right, initially).\n4. visited is a boolean list that tracks whether each cell has been visited.\n5. move_right, move_left, move_down, and move_up are boolean flags that control the allowed directions of movement.\nVisiting each cell\nNow, let\u2019s delve into the core loop that iterates through the matrix: \n# while all elements around it have not been visited\nwhile move_right or move_left or move_down or move_up:\n move_right = can_move_right(visited, current_row, current_col, total_rows, total_cols)\n \n move_left = can_move_left(visited, current_row, current_col, total_rows, total_cols)\n \n move_down = can_move_down(visited, current_row, current_col, total_rows, total_cols)\n \n move_up = can_move_up(visited, current_row, current_col, total_rows, total_cols)\n \n index = to_index(current_row, current_col, total_cols)\n \n result.append(matrix[current_row][current_col])\n \n visited[index] = True\n \nThis while loop continues as long as there\u2019s at least one possible direction to move to an unvisited adjacent cell. Inside the loop:\n1. We check if moving in each direction (right, left, down, up) is valid using helper functions like can_move_right (which you\u2019ll define later).\n2. to_index (another helper function) converts the current row and column to the corresponding index in the visited list.\n3. The current cell\u2019s value is added to the result list.\n4. The current cell is marked as visited in the visited list.\n\u00a0\nThis sets the stage for the implementation of the helper functions and the direction-switching logic within the loop, which will complete the spiral traversal algorithm.\nChoosing the next step\nThe core of our spiral traversal algorithm lies in determining the next cell to visit and adjusting the direction of movement accordingly."}, {"text": "Post Title: Unit Testing with .NET I: A brief overview\nPost Author: Zohair Mehtab\nPost Date: 2024-11-08 11:24:54+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,asp.net,c#,mspec,unit testing,xunit\nPost Content: . Finally, we will go through the xUnit and MSpec unit tests that I have written for the application and compare both testing frameworks. I hope you enjoy reading the article and find it useful. Happy coding! \nReferences \n1.\u00a0https://github.com/zuhairmhtb/UnitTestingWithDotNet\n2.\u00a0https://docs.microsoft.com/en-us/visualstudio/test/unit-test-basics?view=vs-2022\n3. https://www.geeksforgeeks.org/unit-testing-software-testing/\n4. https://www.guru99.com/unit-testing-guide.html\n5. https://www.techtarget.com/searchsoftwarequality/definition/unit-testing#:~:text=Unit%20testing%20is%20a%20software,developers%20and%20sometimes%20QA%20staff.\n6. https://www.testim.io/blog/unit-testing-best-practices/\n7.\u00a0https://dotnet.microsoft.com/en-us/apps/aspnet\n8.\u00a0https://xunit.net/\n9.\u00a0https://github.com/machine/machine.specifications\n10.\u00a0https://www.docker.com/\n11.\u00a0https://visualstudio.microsoft.com/downloads/\n12.\u00a0https://www.nuget.org/ \n"}, {"text": "Post Title: System Overview\nPost Author: Zohair Mehtab\nPost Date: 2024-10-07 11:49:22+00:00\nPost Categories: Documentation,System Architecture\nPost Tags: application layer,data layer,documentation,networking layer,presentation layer,system architecture,system overview\nPost Content:  \nSystem Overview \nA brief overview of the portfolio system \n\"The goal of this system is not to make it simple, but showcase the implementation of different technologies. The goal is to show how different technologies can be combined to build a complex system.\" \n1.0 Overview \nWelcome back! In my previous article here, we delved into the infrastructure and automated deployment process that lays the foundation for my portfolio system. Think of it as setting the stage for a grand performance. Now, it\u2019s time to raise the curtain and reveal the stars of the show \u2013 the applications themselves!\u00a0To make this exploration manageable and engaging, I\u2019ve organized the system into four distinct layers, each with its own unique role:1.\u00a0The Data Layer: This is where the vital information resides, the heart of our system. It houses both custom and third-party applications that act as secure vaults for our data. Think MySQL for WordPress, PostgreSQL for other applications, and so on.2.\u00a0The Application Layer: This layer is where the magic happens! It\u2019s home to the applications that access, transform, and manage the data. Imagine WordPress diligently serving up blog posts, a Node.js application facilitating real-time communication, and Keycloak acting as the vigilant guardian of access control.3.\u00a0The Presentation Layer: This is the face of our system, the captivating interface that greets our users. Currently, it features a sleek Angular application, but exciting additions like Flutter-based mobile apps are on the horizon!4.\u00a0The Networking Layer: This layer ensures smooth communication and efficient traffic flow. It\u2019s the intricate network of pathways connecting our applications, with Docker containers acting as individual hubs, all linked by the Docker network.Now, I know what you\u2019re thinking \u2013 diving deep into every component might feel like a marathon. Fear not! This article provides a scenic overview of the landscape, highlighting the key landmarks and their interactions. Later, we\u2019ll embark on dedicated expeditions to explore each component in detail. So, buckle up and enjoy the journey!Note: I have not created all the components yet. This portfolio is a work in progress and I am actively working on it. Therefore, I will provide GitHub link to all the applications that are currently available. I will keep updating the list here as the number of applications increase. \n2.0 Layers and its components \n Figure: High level overview of the system architecture\nThe diagram above offers a bird\u2019s-eye view of our system architecture \u2013 a snapshot of its current form. Of course, like any dynamic creation, it\u2019s bound to evolve and adapt over time. Rest assured, I\u2019ll keep this visual guide updated as the system grows and matures.As you explore the diagram, you\u2019ll notice the distinct layers and their respective components, each playing a crucial role in the overall symphony. The arrows illustrate the various requests a client might make, tracing the journey of data as it flows through the system."}, {"text": "Post Title: Containerized Django Development III: CSRF and External Applications\nPost Author: Zohair Mehtab\nPost Date: 2024-11-02 00:15:36+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django-cors-headers,django.,docker,graphql,strawberry\nPost Content: . Peeking into our Django logs, we find the culprit: Forbidden (CSRF cookie not set.): /graphql/.This error message highlights the missing piece of the puzzle: the CSRF token. To successfully navigate this security checkpoint, we need to equip our frontend application with a valid CSRF token, just as we did with our Django views.\u00a0Enabling CSRF protection for our GraphQL API involves two crucial steps: \n2.4.1 Set-Cookie header with HTTP response \nWe need to configure Django to include a Set-Cookie header in its HTTP responses, providing the frontend application with a CSRF token to store in its cookie jar. This token acts as a badge of authenticity, allowing the backend to identify requests originating from trusted sources.Unlike our Django views, where the CSRF token is readily available within the HTML form, our GraphQL API deals with JSON responses. These responses lack the necessary structure to embed a CSRF token directly.To overcome this challenge, we can leverage the power of CORS. Since our frontend application is now authorized to send cross-origin requests, we can create a dedicated REST endpoint in our Django backend that serves the sole purpose of providing a valid CSRF token. This endpoint will trigger the Set-Cookie header, equipping our frontend with the necessary token for subsequent GraphQL requests. \n \nTo empower our frontend application with the necessary CSRF protection, we need to make a few adjustments to our Django backend. This involves modifying two key files:1.\u00a0settings.py (left image): We introduce a new setting called CSRF_TRUSTED_ORIGINS. This setting, similar to CORS_ALLOWED_ORIGINS, allows us to specify a list of trusted origins that are permitted to receive CSRF tokens. For our current setup, we\u2019ll keep the values identical to our CORS settings, ensuring consistency between allowed origins for both CORS and CSRF.However, it\u2019s worth noting that these two settings can diverge based on our application\u2019s security requirements. For instance, we might have an external application that only needs read access to our API. In such cases, we can include its origin in CORS_ALLOWED_ORIGINS but exclude it from CSRF_TRUSTED_ORIGINS, effectively preventing it from performing any state-changing actions.2.\u00a0views.py (center image): We add a new endpoint called csrf to our views.py file. This endpoint serves a single purpose: generating and returning a valid CSRF token. By leveraging Django\u2019s built-in csrf module, we can easily create a view that returns a JSON response containing the CSRF token. When the frontend application accesses this endpoint, Django automatically includes the Set-Cookie header, instructing the browser to store the token securely.With these modifications in place, we send a GET request to our newly created /csrf endpoint from our frontend application. As expected, a familiar face greets us in the response headers: the Set-Cookie header, accompanied by a shiny new CSRF token ready to be stored in the browser\u2019s cookie jar (right image).While this marks a significant step forward, we mustn\u2019t forget the lessons learned from our previous exploration of Django views."}, {"text": "Post Title: System Overview\nPost Author: Zohair Mehtab\nPost Date: 2024-10-07 11:49:22+00:00\nPost Categories: Documentation,System Architecture\nPost Tags: application layer,data layer,documentation,networking layer,presentation layer,system architecture,system overview\nPost Content: . This open-source Identity Access Management (IAM) solution from Red Hat provides a comprehensive suite of tools for authentication and authorization. With Keycloak, I can effortlessly implement various access control mechanisms like RBAC, PBAC, and ABAC, ensuring that only authorized users can access specific resources. It also boasts a plethora of advanced authentication features, including OAuth2 provider, two-factor authentication, and JWT authentication. But the true beauty of Keycloak lies in its customizability, allowing me to tailor it precisely to my needs. By offloading the complexities of authentication and authorization to Keycloak, I can focus on designing a robust and secure system.5. The .Net Scheduler: While not depicted in the system diagram, a robust scheduler is essential for automating background tasks. My plan is to develop a CRON scheduler using .NET Core, a powerful and versatile framework that has recently embraced the open-source world. To be honest, the implementation of this scheduler is more about .Net than the scheduler itself. ASP.Net has quite recently introduced itself to the open-source community with the release of .Net Core. I am a fan of .Net technology when it comes to static typed languages and working with a framework backed by an entire enterprise grade ecosystem. However, Microsoft is expensive. Therefore, this project allows me to explore the capabilities of .NET Core in a Linux environment, integrating it seamlessly with other open-source technologies. It\u2019s an exciting opportunity to test whether enterprise-grade .NET applications can be built and deployed without relying solely on the Microsoft ecosystem. \n2.4 The Data Layer \n \nNow, let\u2019s descend into the bedrock of our system \u2013 the data layer! This is where we encounter the raw essence of information, the foundation upon which our entire digital world is built. At its core, digital data is simply a collection of binary digits, meticulously arranged to represent something meaningful. But what truly distinguishes one piece of data from another is how it\u2019s structured, how it relates to other data, and how it\u2019s stored for optimal retrieval and manipulation.When it comes to data and databases, I like to categorize information into three distinct types: \n1.\u00a0Structured Data: This is the well-organized data that fits neatly into rows and columns, like soldiers in formation. It\u2019s easily searchable and analyzable, making it ideal for traditional relational databases. Think of customer records, financial transactions, or product inventories. Some example of such data are user data, role and permission related data, posts and their categories, etc.2.\u00a0Unstructured Data: This is the wild and free-form data that resists rigid structures. It encompasses everything from text documents and emails to images, audio files, and videos. While it can be more challenging to manage and analyze, it holds a wealth of potential insights. Some of the examples of such data are logs, comments, and system metrics.3.\u00a0Blobs: These are the large chunks of binary data, often representing multimedia files or complex objects. They require specialized storage and handling techniques to ensure efficient access and retrieval."}, {"text": "Post Title: Unit Testing with .NET II: Refactoring the code\nPost Author: Zohair Mehtab\nPost Date: 2024-11-14 07:56:28+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,c#,mspec,xunit\nPost Content: .ReadAsStringAsync();\n return content;\n } catch(Exception e)\n {\n Console.WriteLine($\"Error creating response for the request sent to {url}\");\n }\n return null;\n });\n responseGenerator.Wait();\n var contentString = responseGenerator.Result;\n return new HttpResponse() { \n StatusCode = response.StatusCode,\n Content = contentString\n };\n } else\n {\n if (response == null) Console.Write($\"No response returned for url {url}\");\n else if (!response.IsSuccessStatusCode) Console.WriteLine($\"Unsuccessful request sent to {url}. Status code: {((int)response.StatusCode)}\");\n }\n return null;\n }\n public void Dispose()\n {\n if(_client != null) _client.Dispose();\n }\n}\n \nWe will use this wrapper to make the Http Request in our WebScraper class and mock the method in our unit test.We then created a HttpRequestHandler.cs class that creates an HttpClientWrapper wrapper instance and returns the instance. \npublic interface IHttpRequestHandler {\n IHttpClientWrapper CreateClient(IHttpClientFactory clientFactory);\n}\npublic class HttpRequestHandler : IHttpRequestHandler\n{\n public HttpRequestHandler()\n {\n }\n public IHttpClientWrapper CreateClient(IHttpClientFactory clientFactory)\n {\n return new HttpClientWrapper(clientFactory);\n }\n}\n \nWe finally refactored our WebScraper.cs class so that it receives the HttpRequestHandler interface during initialization. We can then use this object to create an implementation of HttpClientWrapper instance to make our requests. \npublic string Scrape()\n{\n string result = null;\n try\n {\n using (var client = _requestHandler.CreateClient(_httpClientFactory))\n {\n var response = client.Get(ScrapingUrl);\n if (response.IsSuccessStatusCode)\n {\n Console.WriteLine(\"Fetched web content:\");\n Console.WriteLine(response.Content);\n result = response.Content;\n }\n else\n {\n var message = \"Client returned an error\";\n if (response != null) message = $\"Client returned a status code of {response.StatusCode}\";\n Console.WriteLine($\"Could not retrieve article of {ScrapingUrl}. {message}\");\n }\n }\n }\n catch (Exception e)\n {\n Console.WriteLine($\"Received an exception: {e.Message}\");\n Console.WriteLine(e.StackTrace);\n } \n return result;\n}\n \nSince we are now dealing with only interfaces in our WebScraper.cs file we can easily write unit tests for our Scrape() method and mock other public methods that we have executed inside it. It will help us to keep the environment of our test isolated and the scope small. \n6.0 Conclusion \nIn this article, we have seen how to write code that is adaptable to unit tests. We have seen what considerations we should keep in mind when writing our classes and methods. Finally, we have also seen the power of interfaces. It is true that in our naive approach we had just one class and now we have to deal with multiple classes. But this is a small tradeoff considering the advantages we will have when we finally write our test cases. In the next article, we will write our MSpec and xUnit test cases. We will finally compare both the testing framework and discuss some of its advantages and limitations. Please feel free to share your feedback and queries. I would love to hear your thoughts and suggestions."}, {"text": "Post Title: The Infrastructure\nPost Author: Zohair Mehtab\nPost Date: 2024-10-07 12:54:34+00:00\nPost Categories: Documentation,Infrastructure Management\nPost Tags: aws,devops,github,github actions,iac,infrastructure,infrastructure as code,terraform\nPost Content: . Think setting up servers, installing software, configuring networks \u2013 the list goes on and on!\nThis is where Infrastructure as Code (IaC) swoops in to save the day!\u00a0\u00a0Instead of doing all that tedious stuff by hand, you write code to automate it. Need a new server? Just push a button! Want to update your software? Another button! IaC lets you manage your entire infrastructure with code, making the whole process faster, more reliable, and way more efficient.\nThink of it like this: IaC takes all those repetitive tasks and turns them into reusable building blocks. Not only does this free up a ton of time, but it also acts as living documentation for your infrastructure. Plus, with everything automated, you\u2019re less likely to run into those pesky inconsistencies that crop up when you\u2019re doing things manually.\nIn short, IaC is all about working smarter, not harder. It\u2019s about defining your infrastructure in a clear, repeatable way, so you can focus on what really matters: building amazing applications!\u00a0\nNow, let\u2019s dive into the engine room of my infrastructure \u2013 the automated deployment process! To achieve seamless and reliable deployments, I\u2019ve enlisted the help of two powerful tools: Terraform and GitHub Actions.\u00a0Here\u2019s a breakdown of how they work together: \n1. Terraform Takes the Lead: All the nitty-gritty details of my infrastructure \u2013 the servers, the network settings, the software \u2013 are defined in Terraform configuration files. This \"infrastructure as code\" approach ensures consistency and makes it easy to manage changes over time.\n2. GitHub Actions - The Deployment Maestro: I've set up dedicated pipelines in GitHub Actions to orchestrate the entire deployment process. These pipelines automatically validate my Terraform code and then deploy the resources to AWS.\n3. Secrets and Variables: Keeping Things Secure: Sensitive information, like API keys and passwords, are securely stored as secrets and variables within GitHub Actions. This keeps my deployment process secure and prevents any accidental leaks.\n4. S3 Bucket: The Statehouse: Terraform state files, which track the current state of my infrastructure, are stored in an AWS S3 bucket. This ensures that deployments are always consistent and traceable.\n5. On-Demand Deployment: I've even added the ability to trigger builds and deployments manually, giving me complete control over when updates are rolled out.\nThis setup provides a robust and efficient way to manage my infrastructure. With Terraform and GitHub Actions working in harmony, I can deploy updates with confidence, knowing that everything is automated, secure, and reliable. It\u2019s like having a dedicated team of experts managing my infrastructure 24/7! \n2.2 A Cozy Home for My Apps: Docker and Friends \n \nTo keep things cost-effective, I\u2019ve decided to house all my applications under one roof \u2013 a single AWS Lightsail instance.\u00a0But don\u2019t worry, they\u2019re not all crammed into one room! Each application lives in its own cozy Docker container, which keeps them nicely isolated and prevents any conflicts."}, {"text": "Post Title: Unit Testing with .NET II: Refactoring the code\nPost Author: Zohair Mehtab\nPost Date: 2024-11-14 07:56:28+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,c#,mspec,xunit\nPost Content: . See you in the next article! \nReferences \n1.\u00a0https://vip3rtech6069.com/post/63/backend/2433/unit-testing-with-net-i-a-brief-overview2.\u00a0https://github.com/zuhairmhtb/UnitTestingWithDotNet/tree/master/UnitTestImplementation3.\u00a0https://github.com/machine/machine.specifications4.\u00a0https://xunit.net/\u00a0\u00a0 \n"}, {"text": "Post Title: The Infrastructure\nPost Author: Zohair Mehtab\nPost Date: 2024-10-07 12:54:34+00:00\nPost Categories: Documentation,Infrastructure Management\nPost Tags: aws,devops,github,github actions,iac,infrastructure,infrastructure as code,terraform\nPost Content: . Think of it like giving each app its own mini-apartment within a larger building.\u00a0\n\u00a0These containers can still chat with each other through the Docker network, kind of like using the building\u2019s intercom system. And to make sure everyone gets their mail (requests, that is!), I\u2019ve set up an Nginx reverse proxy at the front door. It acts like a friendly concierge, directing incoming requests to the right application.\nNow, here\u2019s the real beauty of using Docker: it eliminates those pesky software compatibility issues. Since each application comes bundled with everything it needs within its container, I don\u2019t have to worry about installing different versions of software on the server itself. No more headaches trying to find the perfect match between the application, its dependencies, and the operating system!\nSo, to recap, my infrastructure is like a well-organized apartment building with these key features: \n1. The Building: An AWS Lightsail instance with a public IP address, providing a home for all my applications.\n2. The Concierge: An Nginx reverse proxy, efficiently routing requests to the correct application.\n3. The Apartments: Docker containers, providing isolated environments for each application.\n4. The Utility Room: A dedicated Docker image housing all the essential third-party services, like databases and message brokers, that my applications need to function.\nThis setup provides a streamlined, efficient, and cost-effective solution for hosting my portfolio applications. It\u2019s a testament to the power of containerization and automation! \n3. Leveling Up: The Future of My Infrastructure \nWhile my current setup is pretty slick, I\u2019m always looking for ways to improve! Here are a few ideas brewing for the future, taking IaC and automated deployments to the next level. \n3.1 Centralized Logging and Monitoring \nRight now, I don\u2019t have a central hub for all my logs and metrics. But I\u2019m eyeing AWS CloudWatch to gather this data from all my resources. Imagine a system that seamlessly integrates with my infrastructure repository, collecting logs and displaying them in a centralized dashboard. This would make monitoring a breeze, simplify debugging, and even open doors for automated alerts and reports. Talk about proactive maintenance! \n3.2 Terraform Automation for Everyone \nWith my infrastructure defined as code, I could create a user-friendly tool that automatically generates and updates those Terraform configuration files. Think of a graphical interface where even non-technical users can easily create and manage AWS resources without writing a single line of code. This would democratize infrastructure management and empower everyone to build and deploy with ease. \n3.3 Dynamic and Extensible Codebase \nRemember how I mentioned those scripts for installing Nginx and Let\u2019s Encrypt? Currently, they run manually. But I envision a more dynamic system where these scripts are integrated directly into my Terraform code. Developers could simply specify (using HCL language) whether they want Nginx installed on a particular resource, and voila! \u2013 automation takes care of the rest. This would enhance code reusability and further minimize manual intervention."}, {"text": "Post Title: Unit Testing with .NET II: Refactoring the code\nPost Author: Zohair Mehtab\nPost Date: 2024-11-14 07:56:28+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,c#,mspec,xunit\nPost Content: . \nreadonly IHttpClientFactory _httpClientFactory;\npublic const string ScrapingUrl = \"https://en.wikipedia.org/wiki/Unit_testing\";\npublic WebScraper(IHttpClientFactory httpClientFactory)\n{\n _httpClientFactory = httpClientFactory;\n}\n \nWe then use this object to create a HttpClient object. We can use that to make our HTTP requests using the GetAsync() method. \nHttpResponse get(string url, HttpClient client)\n{\n var task = Task.Run(() => {\n try\n {\n return client.GetAsync(url);\n }\n catch (Exception e)\n {\n Console.WriteLine($\"Http Get request error for url {url}.\");\n Console.WriteLine(e.Message);\n Console.WriteLine(e.StackTrace);\n return null;\n }\n });\n task.Wait();\n var response = task.Result;\n}\n \nWe obtain the result as HttpResponseMessage. If the status code of the HTTP response is 200, we then extract the content of the result as a string. Then we create our own HttpResponse object and return the data. Otherwise, we return null. \nHttpResponse get(string url, HttpClient client)\n{\n // making a request\n var response = task.Result;\n if (response != null && response.IsSuccessStatusCode)\n {\n var responseGenerator = Task.Run(() => {\n try\n {\n var content = response.Content.ReadAsStringAsync();\n return content;\n }\n catch (Exception e)\n {\n Console.WriteLine($\"Error creating response for the request sent to {url}\");\n }\n return null;\n });\n responseGenerator.Wait();\n var contentString = responseGenerator.Result;\n return new HttpResponse()\n {\n StatusCode = response.StatusCode,\n Content = contentString\n };\n }\n else\n {\n if (response == null) Console.Write($\"No response returned for url {url}\");\n else if (!response.IsSuccessStatusCode) Console.WriteLine($\"Unsuccessful request sent to {url}. Status code: {((int)response.StatusCode)}\");\n }\n return null;\n}\n \nThe Scrape() method of the file, is the main public method that creates the client and returns the content from the web. \npublic string Scrape()\n{\n string result = null;\n try\n {\n using (var client = _httpClientFactory.CreateClient())\n {\n var response = get(ScrapingUrl, client);\n if (response.IsSuccessStatusCode)\n {\n Console.WriteLine(\"Fetched web content:\");\n Console.WriteLine(response.Content);\n result = response.Content;\n }\n else\n {\n var message = \"Client returned an error\";\n if (response != null) message = $\"Client returned a status code of {response.StatusCode}\";\n Console.WriteLine($\"Could not retrieve article of {ScrapingUrl}. {message}\");\n }\n }\n } catch(Exception e)\n {\n Console.WriteLine($\"Received an exception: {e.Message}\");\n Console.WriteLine(e.StackTrace);\n }\n return result;\n}\n}\n \n4.0 Problem with this approach \nThis approach seems fine when we are writing code without testing them. But when we start to write unit tests for this code, we may run into different types of issues.One of the best practices for writing unit tests is that we should write our test in a completely isolated environment. When testing a method, we should make sure that other public methods are not executed. Instead, we should mock those methods and return a result to prevent them from executing."}, {"text": "Post Title: Containerized Django Development III: CSRF and External Applications\nPost Author: Zohair Mehtab\nPost Date: 2024-11-02 00:15:36+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django-cors-headers,django.,docker,graphql,strawberry\nPost Content: .However, as I delved deeper into the world of web security, I began to appreciate the intricate dance between CORS, CSRF, and Django\u2019s security mechanisms. Understanding the roles of each player involved transformed this once-daunting task into a manageable and even enjoyable process.This three-part series has been a journey of discovery, not just for my readers, but for myself as well. By meticulously documenting each step and crafting reusable code, I\u2019ve solidified my own understanding of these crucial security concepts. Writing about these intricacies, explaining the challenges and solutions, has helped me internalize the knowledge in a way that simply reading documentation could never achieve.While this series marks the end of our current exploration, it\u2019s by no means the end of my learning journey in web security. This vast and ever-evolving field demands constant learning and adaptation. But armed with the knowledge and tools gained from this series, I feel confident in my ability to face new challenges and build secure, resilient applications.I hope this series has provided valuable insights and guidance to fellow developers navigating the complexities of CORS and CSRF. May your applications remain secure, your users protected, and your coding endeavors filled with the satisfaction of creating robust and reliable web experiences. \nReferences \n1.\u00a0https://github.com/zuhairmhtb/DjangoPlayground\n2.\u00a0https://github.com/zuhairmhtb/ThePortfolioAdmin\n3.\u00a0https://vip3rtech6069.com/post/2/documentation/324/containerized-django-development-i-mastering-docker-cors-and-csrf\n4.\u00a0https://vip3rtech6069.com/post/2/documentation/1703/containerized-django-development-ii-cors-and-views\n5.\u00a0https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Opener-Policy\n6.\u00a0https://cheatsheetseries.owasp.org/cheatsheets/HTTP_Headers_Cheat_Sheet.html\n7.\u00a0https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Mode\n8.\u00a0https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host\n9.\u00a0https://docs.djangoproject.com/en/5.1/ref/settings/\n10.\u00a0https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie#samesitesamesite-value\n11.\u00a0https://pypi.org/project/django-cors-headers/ \n"}, {"text": "Post Title: Containerized Django Development II: CORS and Views\nPost Author: Zohair Mehtab\nPost Date: 2024-11-01 20:55:54+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django,docker\nPost Content: . Armed with our newfound knowledge, we prepare to explore the challenges and solutions for securing API interactions in a world dominated by JavaScript frontends and token-based authentication. \n4.0 Conclusion \nWhile our initial goal was to cover the complete implementation of CORS and CSRF for our GraphQL API in this article, it seems we\u2019ve reached the limits of our current platform (WordPress Elementor is the culprit here). Fear not, for this is merely a pause in our journey, not the end!In this part, we\u2019ve laid the groundwork for understanding CORS and CSRF, exploring their core concepts through practical examples and experiments. We\u2019ve ventured into the world of cross-origin requests, encountering the challenges and solutions associated with securing our Django application from unauthorized access.We\u2019ve witnessed the browser\u2019s role as a CORS enforcer, the importance of configuring ALLOWED_HOSTS, and the intricacies of preflight requests. Through these explorations, we\u2019ve gained valuable insights into diagnosing and resolving CORS-related issues.But our adventure doesn\u2019t end here! In the next installment of this series, we\u2019ll delve deeper into the realm of GraphQL, exploring the implementation of CSRF protection for API endpoints. We\u2019ll uncover the techniques for securing POST requests from external applications while maintaining a robust security posture.Stay tuned for the final chapter, where we\u2019ll bring together all the pieces of the puzzle and achieve a comprehensive solution for securing our Django GraphQL API. Until then, happy coding and may your applications remain safe from the clutches of cross-origin attacks! \nReferences \n1.\u00a0https://vip3rtech6069.com/post/2/documentation/324/containerized-django-development-i-mastering-docker-cors-and-csrf\n2.\u00a0https://github.com/zuhairmhtb/DjangoPlayground\n3.\u00a0https://github.com/zuhairmhtb/ThePortfolioAdmin\n4. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cross-Origin-Opener-Policy\n5. https://cheatsheetseries.owasp.org/cheatsheets/HTTP_Headers_Cheat_Sheet.html\n6. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Sec-Fetch-Mode\n7. https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Host\n8. https://docs.djangoproject.com/en/5.1/ref/settings/ \n"}, {"text": "Post Title: Unit Testing with .NET I: A brief overview\nPost Author: Zohair Mehtab\nPost Date: 2024-11-08 11:24:54+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,asp.net,c#,mspec,unit testing,xunit\nPost Content: .The reason for this approach is that, while testing our subject we should not be concerned about how the logic of other dependencies is being executed. It makes the test simpler and decreases the number of scenarios that we need to test. All we need to be concerned about is, what type of output the dependency is expected to return for the input that our subject provides.For example, in this case, we can create the following context when the input \u2018x\u2019 is positive and the input \u2018y\u2019 is negative:1.\u00a0When the test executes \u2018IsPositive(x)\u2019 return \u2018true\u2019.2.\u00a0When the test executes \u2018IsPositive(y)\u2019 return \u2018false\u2019.In our context, we have defined what the output of \u2018IsPositive\u2019 should be for \u2018x\u2019 and what it should be for \u2018y\u2019. So while testing, when the method \u2018IsPositive(x)\u2019 is executed, it will immediately return \u2018true\u2019 instead of actually executing the method. This is known as \u2018Mocking\u2019 a method. It helps us remove dependencies of the subject and only test the logic inside the subject. In the same way we can mock class objects that our subject uses to perform a task. \n5.0 Structure of a Unit test \nA unit test generally has three parts. When a unit test starts executing, it executes in the following sequence:5.1 ArrangeThis is the part where we create the context for our unit tests. We mock the methods which are out of the scope of our test, we mock dependency and inject those mock dependencies to our subject so that the subject uses our mock instead of the actual ones. In this way we can control what the mock should return without being executed. We also define what our mocks should return when a specific method of the mock is executed. When a unit test starts executing it first creates the context of the test. In xUnit we call this \u2018Arrange\u2019 and in MSpec we call this \u2018Establish\u2019.5.2 ActThis is the part where unit test executes the method or subject that we are testing. In xUnit this is known as \u2018Act\u2019 and in MSpec this is known as \u2018Because\u2019.5.3 AssertIn this part we verify that for each input the subject returns the output that we expect. It also verifies that the mock methods which were supposed to be executed for the input, was actually executed with the desired inputs provided to the mock methods. For our above example, we can verify that the \u2018IsPositive\u2019 was executed twice \u2014 once with \u2018x\u2019 as the input and a second time with \u2018y\u2019 as the input. \n6.0 Conclusion \nIn this article, we have gone through the concept, purpose, and structure of unit tests. In the next articles, we will go through a simple .NET console application that fetches the contents of a web page and displays it in the console. It uses an Http request to fetch the data. We will go through different interfaces that we create for the application and the purpose of those interfaces."}, {"text": "Post Title: Traversing a spiral matrix\nPost Author: Zohair Mehtab\nPost Date: 2024-11-17 16:15:52+00:00\nPost Categories: Arrays,Blogs,Data Structure\nPost Tags: array,competitive programming,data structure,matrix,spiral,traversal\nPost Content: .0 Conclusion \nIn this blog post, we embarked on a journey to unravel the intricacies of spiral matrix traversal. We explored the fundamental concepts behind this technique, dissecting the logic and steps involved in navigating a 2D array in a spiral pattern. By leveraging a classic LeetCode problem as our guide, we demonstrated how to translate these concepts into a working algorithm, complete with code examples and explanations.\nMastering spiral traversal equips programmers with a valuable tool for solving a variety of problems involving 2D matrices. From image processing and data compression to game development and pathfinding, this technique finds applications in diverse domains. By understanding the principles and implementation details discussed in this post, you\u2019re now well-prepared to tackle challenges that demand efficient and systematic matrix navigation.\nAs you continue your programming journey, remember that exploring different traversal techniques broadens your problem-solving horizons. So, keep experimenting, keep learning, and continue to unlock the fascinating world of algorithms and data structures! \nReferences \n1.\u00a0https://leetcode.com/problems/spiral-matrix/submissions/1455337095/ \n"}, {"text": "Post Title: System Overview\nPost Author: Zohair Mehtab\nPost Date: 2024-10-07 11:49:22+00:00\nPost Categories: Documentation,System Architecture\nPost Tags: application layer,data layer,documentation,networking layer,presentation layer,system architecture,system overview\nPost Content: .Now, I must confess \u2013 for the sake of clarity, I\u2019ve omitted one vital element from this architectural overview: logging. In my experience, logging is like a trusty compass, guiding you through the complexities of a growing system. Believe me, I\u2019ve lost myself in the depths of production debugging, spending hours searching for the source of an issue. With proper logging, those hours can shrink to mere minutes or even seconds! But fear not, logging will have its own dedicated article where we can delve into its importance and intricacies.In this section, we\u2019ll take a closer look at each layer, examining its components and the rationale behind my technology choices. Get ready for a fascinating journey through the inner workings of the portfolio system! \n2.1 The Networking Layer \n \nWelcome to the front lines! The networking layer acts as the vigilant gatekeeper of our system, the first point of contact for every incoming request. It\u2019s a critical zone where we establish a robust security perimeter, ensuring only authorized traffic proceeds further.Rather than reinventing the wheel, I\u2019ve enlisted the help of several open-source allies, each playing a specific role in this carefully orchestrated sequence. Think of it as a series of checkpoints, each request passing through one layer before reaching the next. Here\u2019s a closer look at our security personnel: \n1.\u00a0AWS Lightsail Firewall: Our applications reside within the comfy confines of AWS Lightsail instances, which come equipped with a handy built-in firewall. Embracing the principle of least privilege, I can meticulously control access to specific ports and endpoints, ensuring only the necessary doors are open. While Terraform\u2019s support for managing these rules is still evolving, I\u2019m happy to take the reins with a bit of manual configuration.2.\u00a0Nginx Reverse Proxy: Ah, Nginx, my trusty companion! This versatile tool excels not only as a web server but also as a masterful traffic conductor. Over the years, it\u2019s become my go-to reverse proxy, thanks to its user-friendly configuration, open-source nature, and vibrant community support. Nginx stands ready to greet every incoming request (if permitted by Lightsail, of course), carefully determining its legitimacy, destination, and communication protocol. It\u2019s also a prime location for logging all traffic, providing valuable insights into our system\u2019s activity. Each public-facing application enjoys its own domain (or subdomain), a dedicated Nginx configuration file, and a secure SSL certificate. Nginx expertly maps authenticated requests to their appropriate routes, ensuring smooth and efficient delivery. And the best part? This is just the beginning! I have ambitious plans to introduce load balancers in the future, further enhancing our system\u2019s resilience and scalability.3.\u00a0The Docker Network: Our backend applications reside in their own private Docker containers, like individual apartments within a larger complex. This isolation not only prevents conflicts but also simplifies deployment and promotes modularity. But don\u2019t worry, these applications aren\u2019t solitary creatures! They constantly communicate with each other through an external Docker network, a bustling hub of activity."}, {"text": "Post Title: Containerized Django Development III: CSRF and External Applications\nPost Author: Zohair Mehtab\nPost Date: 2024-11-02 00:15:36+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django-cors-headers,django.,docker,graphql,strawberry\nPost Content: .\u00a0The error message, however, offers a cryptic clue:If an opaque response serves your needs, set the request\u2019s mode to \u2018no-cors\u2019 to fetch the resource with CORS disabled. \nThis\u00a0introduces us to the\u00a0sec-fetch-mode\u00a0header and the concept of \u201copaque responses.\u201d While we won\u2019t delve into the technical depths here (you can find more details in the resources section), we can experiment with this mode using the following code:await\u00a0fetch(\u201chttp://localhost:8000\u201d, {mode:\u00a0\u2018no-cors\u2019});This modified request, bypassing CORS restrictions, elicits a successful response from our Django application. However, the response remains shrouded in mystery, an \u201copaque\u201d entity devoid of accessible data.This highlights a common misconception among new frontend developers. The CORS error, with its browser-side manifestation, can mislead one into believing the issue lies within the frontend code. But as our Django logs reveal, the request does reach the backend, triggering a response as shown in the bottom right image. This happens because even though our request reaches the backend, the browser prevents it as the HTTP response does not contain the url of our frontend application in its header that allow CORS. This is usually done by sending an OPTIONS request (or a pre-flight request) before the actual request.This is where CORS steps in as the gatekeeper of cross-origin access. Without explicitly granting permission to our external application, even a simple GET request is denied entry.\u00a0To overcome this CORS hurdle, we have two primary options:1.\u00a0The Open Door Policy: Allow requests from all external applications. This approach, while simple to implement, leaves our API vulnerable to any and all cross-origin requests. It\u2019s a risky strategy best suited for scenarios where open access is desired.2.\u00a0The Guest List: Curate a list of trusted origins and grant access only to those specific applications. This approach offers a more secure and controlled environment, ensuring that only authorized parties can interact with our API.For our purposes, we\u2019ll adopt the more secure \u201cguest list\u201d approach, explicitly granting access to our external application through our settings.py file. This will demonstrate how to fine-tune CORS settings and establish secure communication channels with trusted external applications.\u00a0 \n2.2.1 A Detour - Allowed Hosts \nBefore enabling CORS, we will take a detour \u2013 as we delve deeper into our CORS experiment, we encounter a perplexing error message:Invalid HTTP_HOST header: \u2018172.22.0.4:8000\u2019. You may need to add \u2018172.22.0.4\u2019 to ALLOWED_HOSTS.The IP address (172.22.0.4) in your log may be different but the message will be the same. This seemingly innocuous message raises two intriguing questions:1.\u00a0The Identity Crisis: Why does the error message point to 172.22.0.4:8000 when our application resides at localhost:8000?2.\u00a0The Host Conundrum: What exactly is ALLOWED_HOSTS, and how does it differ from CORS?Unmasking the Docker HostThe answer to our first question lies within the realm of Docker. Our Django application, nestled within a Docker container, is assigned a private IP address (172.22.0.4 in this case) by the Docker network."}, {"text": "Post Title: Containerized Django Development I: Mastering Docker, CORS, and CSRF\nPost Author: Zohair Mehtab\nPost Date: 2024-10-31 21:02:57+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django,docker,graphql,strawberry\nPost Content: . This brings some fantastic benefits:1.\u00a0Reduced Network Overhead: Only fetch the data you need, minimizing those costly network trips.2.\u00a0Improved Client Performance: Less data to process means a happier and faster client-side experience.3.\u00a0Efficient Data Retrieval: Grab multiple types of information with a single network call, reducing those back-and-forth requests.I\u2019m a big fan of GraphQL, especially for projects where I anticipate a growing and complex set of API endpoints. It keeps things organized and efficient.While Graphene has been a popular choice for GraphQL in Django, I recently discovered that it doesn\u2019t have the best support for asynchronous operations. That\u2019s where Strawberry comes in! This modern Python and Django library embraces async and supports the latest GraphQL specifications, including subscriptions and websockets. Plus, it has a vibrant and growing community.To get started with Strawberry, we\u2019ll need to ensure ASGI is enabled in our Django project (remember Daphne from earlier?). Then, we\u2019ll add Strawberry to our requirements.txt file. Next, we\u2019ll define our GraphQL schema and resolvers in a schema.py file. Don\u2019t worry, I\u2019ll provide some helpful resources to guide you through this process.Once our schema is ready, we\u2019ll update our urls.py file to include the route to our GraphQL endpoint. And finally, we can test everything out using a GraphiQL IDE. It\u2019s like a playground for your GraphQL queries! \n6.0 CORS and CSRF: Guarding the Gates of Our API \nLet\u2019s talk security! When building web applications, it\u2019s crucial to protect our API from unauthorized access and malicious attacks. That\u2019s where Cross-Origin Resource Sharing (CORS) and Cross-Site Request Forgery (CSRF) protection come in.CORS acts like a gatekeeper, controlling which external applications (those hosted on different domains) can access our Django server. Think of it as a guest list for our API. We specify the allowed origins in the HTTP headers of our server\u2019s responses, ensuring that only trusted applications can access our precious resources.CSRF, on the other hand, protects against a more insidious type of attack. Imagine an attacker exploiting a user\u2019s authenticated session to perform unauthorized actions on their behalf. Not good! CSRF protection acts like a security guard, verifying that requests genuinely originate from the authenticated user and not a malicious third party. Django provides built-in CSRF protection through middleware, making our lives easier.But here\u2019s where things get interesting. Let\u2019s say we have a frontend application, like Angular, that interacts with our GraphQL API. In this scenario, the frontend acts on behalf of the user, making requests to our backend. Our backend needs to be smart enough to distinguish between requests coming directly from the user and those coming from an authorized application (like our Angular frontend).To achieve this delicate balance, our application will perform the following steps:\u00a0 \n \n1.\u00a0Setting the Stage with CORS Headers:\u00a0First, we need to tell our Django application who\u2019s allowed to knock on its door. We do this by configuring the CORS_ALLOWED_ORIGINS setting in our settings.py file."}, {"text": "Post Title: Containerized Django Development I: Mastering Docker, CORS, and CSRF\nPost Author: Zohair Mehtab\nPost Date: 2024-10-31 21:02:57+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django,docker,graphql,strawberry\nPost Content: . In this section, we\u2019ll explore how to set up these powerful tools within our Dockerized Django environment. I\u2019ll be using VS Code for this demonstration, but the concepts apply to other IDEs like IntelliJ IDEA as well. You\u2019ll just need to adjust the configuration files accordingly.Hot reloading is like magic for developers. It allows you to see the effects of your code changes instantly without manually restarting the server. Thankfully, Django comes with hot reloading enabled by default. As long as you\u2019ve correctly mounted your application volume in your docker-compose setup, you can make changes to your Django files, and they\u2019ll be reflected immediately in your running application. Talk about a productivity boost!\u00a0 \n \nNow, let\u2019s talk debugging. VS Code offers a handy feature called \u201cRemote Explorer\u201d that lets you open your source code directly from within a Docker container. However, we\u2019re going to take a slightly different approach here. We\u2019ll use the debugpy library, which allows us to attach a debugger to our running Docker container.Here\u2019s how it works:1.\u00a0We\u2019ll configure debugpy in our manage.py file to listen on a specific port within our Docker container. This will only be enabled in debug mode or when we explicitly want to use breakpoints.2.\u00a0We\u2019ll then set up our launch.json file in VS Code to connect the debugger to that port.This way, when we set a breakpoint in our VS Code editor, the message is relayed to our Dockerized application through debugpy, pausing execution exactly where we want it. \nfrom django.conf import settings\ndef main():\n \"\"\"Run administrative tasks.\"\"\"\n os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'django_playground.settings')\n # if settings.DEBUG:\n # if os.environ.get('RUN_MAIN') or os.environ.get('WERKZEUG_RUN_MAIN'):\n # import debugpy\n # debugpy.listen((\"0.0.0.0\", 4000))\n # debugpy.wait_for_client()\n # print('Attached!')\n try:\n from django.core.management import execute_from_command_line\n \nNow, here\u2019s the catch. In my experience, there\u2019s a bit of a conflict between hot reloading and debugging in this setup. When both are enabled, any changes I make trigger a reload, which disconnects the debugger and throws a runtime exception. A bit annoying, right?My workaround is to comment out the debugging code in manage.py when I need the seamless flow of hot reloading. I only enable it when I need to meticulously step through the code and don\u2019t anticipate making frequent changes. It\u2019s not ideal, but it gets the job done for now.Hopefully, this little quirk will be ironed out in future Django or Docker updates. Fingers crossed! \n5.0 GraphQL with a Strawberry Twist: Embracing Async \nNow, let\u2019s add some GraphQL magic to our Django application. I\u2019m going to assume you\u2019re already familiar with the wonders of GraphQL and the power of ASGI. If you need a refresher, I\u2019ve included some helpful links in the resources section.For those who haven\u2019t experienced the joy of GraphQL, it\u2019s a query language that gives you precise control over the data you fetch from a server. Think of it as ordering your data \u00e0 la carte instead of getting a fixed-price menu."}, {"text": "Post Title: Unit Testing with .NET I: A brief overview\nPost Author: Zohair Mehtab\nPost Date: 2024-11-08 11:24:54+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,asp.net,c#,mspec,unit testing,xunit\nPost Content:  \nUnit Testing with .Net I \nA brief overview of unit testing \n1.0 Overview \nWe were taught a lot of stuff at our university \u2014 starting from the basic concepts of programming languages followed by data structure and algorithms leading up to the development of web applications.Then I started developing personal web application projects for fun and learning purposes. But I always concentrated on developing the application itself. Finally, when I started my job, I got involved in developing web applications that follow modern industry standards.One thing I learned from my work experience is that the development of the application itself is just a part of the entire system. An important aspect of programming is testing our code. We usually learn the concept of unit testing at universities but the topic is not prioritized as it should be.When I interact with other programmers in our country, it becomes clearer that no matter how good a person can code, many among them always struggle to write tests for their code. I can only assume that the reason for it is our unfamiliarity with unit tests and lack of practice in writing unit tests. Because of it, we always struggle to write unit tests when we start working with production-grade applications.During my early days of writing unit tests, one of the two questions that bothered me the most was, \u201chow should I construct my test cases, and what should I exactly test?\u201d The other question was, \u201chow many scenarios do I cover before I can say that I have completed writing enough test cases for one functionality? At what point do test cases become too many test cases?\u201dI have been developing monolithic and microservice-based web applications with ASP.Net framework for a while now. In this and the upcoming articles, I would therefore like to discuss the basics of unit testing and show the comparison between two popular testing frameworks of DotNet \u2014 xUnit and Machine.Specifications (MSpec). I have developed a console application containing the test cases that I will discuss in this article. To easily run the application, I have dockerized the entire application. All you need is Docker Desktop and Visual Studio (Preferably 2019 or above). You can find my code here. You can clone the repository, open the solution in Visual Studio, and hit the \u2018Run\u2019 button. Docker will download the required NuGet packages and run the application. To run the test cases, open \u2018Test Explorer\u2019 and just hit the \u2018Run all\u2019 button. Both MSpec and XUnit tests should run properly. \n2.0 What are Unit tests \nUnit testing is a software testing process in which we logically break down our code into small sections, usually a method or a module. These small sections are known as units. We then write code to test these units."}, {"text": "Post Title: CRON Scheduler with .Net Core\nPost Author: Zohair Mehtab\nPost Date: 2024-11-08 10:45:02+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,asp.net,c#,cron,quartz,quartz.net,scheduler\nPost Content:  \nCRON Scheduler with .Net Core \nImplementing a scheduler with dynamic CRON schedules \n1.0 Overview \nSchedulers are applications that perform some tasks periodically. One of the terms that most developers are familiar with regarding schedulers is \u2014 CRON jobs. It is a very useful application as it can run in the background.I was recently trying to build a scheduler in ASP.Net core using Quartz.Net which is an open-source job scheduling system. When building a scheduler with .Net it is a common practice to configure the scheduler in the program.cs file. We specify the schedule of a task once. When we need to change the schedule we normally update the schedule and restart the application. However, there are scenarios when we want to store the job schedules in the database and update them from a different application \u2014 for example via an admin backend application. In this case, we would not want to manually restart the application every time someone changes the schedule in the database. With Quartz, if we want to update schedules without restarting the application, the scenario becomes a little tricky.In this article, I will discuss the approaches that I followed to build a scheduler and add the functionality to update the schedules without stopping the scheduler. You can find the complete code on GitHub. Please click the link here to view the source code. I am assuming that everyone reading this article is familiar with .Net Core and schedulers in general. \n2.0 An Overview of Quartz.Net \nFirst I would like to give a brief overview of the different features of Quartz that we will use and the role that it will play in our application. If you would like to read in details more about these features, I highly recommend reading the documentation of Quartz.Net.In Quartz, each task that we perform is called a Job. Each job can either run just once during a specific date and time, or it can run periodically at specified intervals. A scheduling system can have multiple jobs that get executed at different times and at different rates. Each job has its name and a group name. Multiple jobs can belong to the same group. To uniquely identify a job, we use a Job key which is a combination of the job\u2019s name and its group name.Next, we specify a schedule for each job \u2014 we specify when and how often the job will get executed. We can also specify a start and an end time. The schedule is generally written as a CRON expression. In Quartz, we call this schedule \u2014 trigger. Like a job, a trigger has a name and a group name. We uniquely identify a trigger using a trigger key which is a combination of its name and the name of the group that it belongs to.A scheduler is the manager of the scheduling system. We assign multiple jobs to the scheduler and attach a schedule to each job."}, {"text": "Post Title: CRON Scheduler with .Net Core\nPost Author: Zohair Mehtab\nPost Date: 2024-11-08 10:45:02+00:00\nPost Categories: Backend,Blogs\nPost Tags: .net,.net core,asp.net,c#,cron,quartz,quartz.net,scheduler\nPost Content: . In this way, if we create a new worker job, all we need to do is create the job class, update the appsettings.json to store the cron expression, and add the worker\u2019s class name to the list in ZookeeperJobFactory class.6.\u00a0InvoiceNotificationJob.cs: This is a sample worker job. It just prints the date and time when the job gets executed. But we can create our worker jobs and add the code to specify the task that the worker will perform.7.\u00a0BackOrderNotificationJob.cs: This is also a sample worker job like InvoiceNotification.8.\u00a0SchedulerConfiguration.cs: In this file, we add the code to configure the scheduler for the first time.When configuring the jobs, we will add dependency injection so that the jobs are singleton classes. When creating the instance, we will fetch an instance from the ServiceProvider so that we get the same instance of the job every time and do not create a new instance of the class every time. \n3.1 Explanation of the Source Code \nFilename: Program.cs \npublic static IHostBuilder CreateHostBuilder(string[] args)\n{\n return Host.CreateDefaultBuilder(args)\n .UseWindowsService()\n .ConfigureServices(async services => {\n services.AddOptions();\n ServiceRegistration.AddServices(services);\n await SchedulerConfiguration.Configure(services);\n });\n}\n \nWe configure the application in this method. We create a Host for the application and configure our services and scheduler. We call the method \nServiceRegistration.AddServices(services);\n \nto configure our services and the method \nawait SchedulerConfiguration.Configure(services);\n \nto configure and start our scheduler.Filename: ServiceRegistration.cs \npublic static void AddServices(this IServiceCollection services)\n{\n services.AddSingleton();\n services.AddSingleton();\n services.AddSingleton();\n}\n \nIn this method, we configure our ServiceProvider and specify that our jobs should be created as singleton instances. When we need to get an instance of the job, we make a call to the ServiceProvider so that it returns the same instance every time.Filename: SchedulerConfiguration.cs \npublic static async Task Configure(IServiceCollection services)\n{\n try\n {\n var serviceProvider = services.BuildServiceProvider();\n var factory = new StdSchedulerFactory();\n var scheduler = await factory.GetScheduler();\n scheduler.JobFactory = new ZookeeperJobFactory(serviceProvider, scheduler);\n await ConfigureWorkers(scheduler);\n await ConfigureZookeeper(scheduler);\n // Start Scheduler\n Console.WriteLine(\"Starting Scheduler\");\n await scheduler.Start();\n await Task.Delay(TimeSpan.FromSeconds(1));\n \n } catch(Exception e)\n {\n Console.WriteLine(\"Error configuring scheduler. \" + e.Message);\n }\n}\n \nThis is the method that configures our scheduler when the application starts. First, we create an instance of \u2018StdSchedulerFactory\u2019. This is the standard scheduler factory provided by Quartz. It returns an instance of the scheduler. Next, we assign a JobFactory to the scheduler so that whenever the scheduler decides to execute a job, it fetches an instance of the job from the JobFactory. We finally configure the Worker jobs and the Zookeeper job. Then we start the scheduler. We pass the same scheduler to both type of jobs so that all jobs reside within the same scheduler. \nstatic async Task ConfigureZookeeper(IScheduler zookeeperScheduler)\n{\n // Configure Scheduler for Zookeeper\n var zookeeperTriggerTime = AppSettings.GetValue(AppSettings.ZookeeperTriggerTime);\n Console."}, {"text": "Post Title: Containerized Django Development III: CSRF and External Applications\nPost Author: Zohair Mehtab\nPost Date: 2024-11-02 00:15:36+00:00\nPost Categories: Application Layer,Documentation\nPost Tags: asgi,containerization,cors,csrf,daphne,deployment,development,django-cors-headers,django.,docker,graphql,strawberry\nPost Content:  \nContainerized Django Development: Part 3 \nCSRF and External Applications \n1.0 Overview \nWelcome back, security aficionados! This is the third and final chapter of our series on implementing CORS and CSRF with Django. If you have not read the previous two articles, I would highly recommend reading them first. I have provided the links to those articles below.\u00a0In the previous chapter of our containerized Django development saga, we conquered the realm of CORS, ensuring our application could safely communicate with trusted external origins. Now, we turn our attention to a different beast: Cross-Site Request Forgery (CSRF).CSRF, a cunning adversary that exploits user trust, can wreak havoc on unsuspecting web applications. Imagine a malicious actor silently hijacking a user\u2019s authenticated session to perform unauthorized actions. Not a pleasant scenario, is it?In this installment, we\u2019ll delve into the depths of CSRF protection, exploring how Django\u2019s built-in mechanisms and the django-cors-headers library work in tandem to safeguard our GraphQL API. We\u2019ll unravel the mysteries of CSRF tokens, uncover potential vulnerabilities, and equip ourselves with the knowledge to build a fortress against these insidious attacks.Get ready to don your security armor and embark on a quest to fortify your Django application against the ever-present threat of CSRF! \n2.0 Venturing Beyond the Walls: CORS and CSRF with External Applications \nOur journey into Django\u2019s security measures now takes us beyond the confines of our own application. We\u2019ll explore how CORS and CSRF come into play when external applications, residing in different domains, attempt to interact with our Django backend. \n2.1 The Setup \nThe beauty of this exploration is that it doesn\u2019t require a full-fledged frontend application or even a GraphQL endpoint. Any external webpage with a console will suffice, allowing us to send cross-origin requests and witness the interplay of CORS and CSRF.Our experiment will unfold in two stages:1.\u00a0The Unprotected Realm: We\u2019ll begin with a clean slate, using Django\u2019s default settings without any custom CORS or CSRF configurations. This will expose the vulnerabilities of an unprotected API to cross-origin requests.2.\u00a0Building the Fortress: We\u2019ll then gradually introduce CORS and CSRF configurations, observing how each setting influences the response and strengthens our API\u2019s defenses.This two-pronged approach will provide a clear understanding of how CORS and CSRF work in tandem to protect our application from unauthorized cross-origin access. \n2.2 The Experiment (GET requests from external app) \n \nWith our external application primed and ready, we fire off a GET request towards our Django backend. But instead of a warm welcome, we\u2019re met with a formidable barrier:Access to fetch at \u2018http://localhost:8000/\u2019 from origin \u2018https://vip3rtech6069.com\u2019 has been blocked by CORS policy.The browser,\u00a0acting as a vigilant border guard, throws up a CORS error, preventing our external application from accessing the requested resource. Our request headers, as sparse as a desert landscape, offer no credentials to bypass this security checkpoint."}]