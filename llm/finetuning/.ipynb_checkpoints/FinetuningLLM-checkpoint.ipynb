{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e206b29-c100-4edf-b5cb-3deff20c6f08",
   "metadata": {},
   "source": [
    "# Fine-tuning LLM for Wordpress Posts\n",
    "\n",
    "This document contains the code to fine-tune Gemma with Wordpress Posts for conversation as a chatbot assistant.\n",
    "\n",
    "**Database: Weaviate**\n",
    "\n",
    "**Collection: Post**\n",
    "\n",
    "**Schema**\n",
    "\n",
    "```\n",
    "class Post(WeaviateCollection):\n",
    "    id:str\n",
    "    postId:str\n",
    "    postTitle:str\n",
    "    postExcerpt:str\n",
    "    postContent:str\n",
    "    postDate:datetime\n",
    "    postAuthor:str\n",
    "    postCategories:typing.Optional[str]\n",
    "    postTags:typing.Optional[str]\n",
    "    postUrl:typing.Optional[str]\n",
    "    postSequence:typing.Optional[int]=1\n",
    "    isDeleted:typing.Optional[bool]=False\n",
    "\n",
    "    def get_embedding(self):\n",
    "        return [0.12345] * 1536\n",
    "```\n",
    "\n",
    "## 1.0 Steps to create embeddings\n",
    "\n",
    "We will generate embeddings externally using Gemma's tokenizer and store it in weaviate. This will enable not only to perform semantic search in the databaset, it will also keep the vectors prepared and stored reducing the time to generate vectors each time a request is made.\n",
    "\n",
    "1. The following parameters will be included in the embedding:\n",
    "\n",
    "    - postContent: Main content of the post\n",
    "    - postTitle: Title of the post\n",
    "    - postAuthor: Author of the post\n",
    "    - postDate: Publish date of the post\n",
    "    - postCategories: Categories that the post belongs to\n",
    "    - postTags: Tags related to the post\n",
    "\n",
    "    \n",
    "\n",
    "2. The following type of embeddings will be available for the system.\n",
    "\n",
    "    - **Fine-tuning:** When fine-tuning the model, the embedding will contain post contents with some metadata. The text needs to be conversational.\n",
    "    - **Query:** When querying the LLM, the text that will be converted to embedding will be different from the ones that are used for fine-tuning. The text will contain instructions, user prompt, and a context. The combined text will form the embedding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140283fd-f7a2-47f7-99f8-0c5287a88633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "\n",
    "import weaviate\n",
    "import typing\n",
    "from weaviate.connect import ConnectionParams\n",
    "from weaviate.classes.init import AdditionalConfig, Timeout, Auth\n",
    "from weaviate.classes.config import DataType, Configure, Property\n",
    "from weaviate.classes.query import Filter\n",
    "from abc import ABC, abstractmethod\n",
    "import strawberry\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import json\n",
    "import httpx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f2a66c-0397-46c9-a9fb-ea86629c5582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add HF_HOME and HF_TOKEN to env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb1bb6b-b370-4336-8655-e90f3cf1a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03209b7c-6100-4d33-9452-aca3b98581b6",
   "metadata": {},
   "source": [
    "## 2.0 Weaviate configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7612ef-2ea1-48f0-9e85-b2a00a47adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_host = \"localhost\"\n",
    "weaviate_port = 50050\n",
    "weaviate_grpc_port = 50051\n",
    "weaviate_scheme = \"http\"\n",
    "weaviate_user = \"admin@vip3rtech6069.com\"\n",
    "weaviate_key = \"admin123\"\n",
    "\n",
    "weaviate_connection_params = ConnectionParams.from_params(\n",
    "    http_host=weaviate_host,\n",
    "    http_port=weaviate_port,\n",
    "    http_secure=weaviate_scheme == \"https\",\n",
    "    grpc_host=weaviate_host,\n",
    "    grpc_port=weaviate_grpc_port,\n",
    "    grpc_secure=weaviate_scheme == \"https\"\n",
    ")\n",
    "weaviate_auth_secret = Auth.api_key(weaviate_key)\n",
    "weaviate_additional_config = AdditionalConfig(\n",
    "    timeout=Timeout(init=30, query=60, insert=120),  # Values in seconds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1086cb5-dbb6-4f39-b116-ebbdb3a6ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_client = weaviate.WeaviateAsyncClient(\n",
    "    connection_params=weaviate_connection_params,\n",
    "    auth_client_secret=weaviate_auth_secret,\n",
    "    additional_config=weaviate_additional_config,\n",
    "    skip_init_checks=True\n",
    ")\n",
    "\n",
    "await async_client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff0e6648-61b7-438b-9916-5f9eeea6b3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await async_client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c8aa4b8-01f9-4510-ab52-fd80a7acbea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-11-22T12:03:13.000148'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9090914d-9ec5-4c36-845e-b59462f1d624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await (async_client.collections.get(\"Post\")).length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dc946-b6dd-41dc-8e4e-699f8fa21796",
   "metadata": {},
   "outputs": [],
   "source": [
    "await async_client.collections.delete(\"Post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f74e6c-0ec8-4f9e-8ffa-09803c11951d",
   "metadata": {},
   "source": [
    "## 3.0 Wordpress configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9554b43e-4cd0-4b27-8bf5-abe822af5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeaviateCollection(ABC):\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def from_dict(data:dict)->typing.Type[typing.Any]:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def to_dict(data:typing.Any)->dict:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_embedding(self)->typing.List[float]:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def get_field_mapping()->dict:\n",
    "        pass\n",
    "    \n",
    "\n",
    "@strawberry.type\n",
    "class Post(WeaviateCollection):\n",
    "    id:str\n",
    "    postId:str\n",
    "    postTitle:str\n",
    "    postExcerpt:str\n",
    "    postContent:str\n",
    "    postDate:datetime\n",
    "    postAuthor:str\n",
    "    postCategories:typing.Optional[str]\n",
    "    postTags:typing.Optional[str]\n",
    "    postUrl:typing.Optional[str]\n",
    "    postSequence:typing.Optional[int]=1\n",
    "    isDeleted:typing.Optional[bool]=False\n",
    "\n",
    "    @staticmethod\n",
    "    def to_dict(post):\n",
    "        return {\n",
    "            \"id\": post.id,\n",
    "            \"postId\": post.postId,\n",
    "            \"postTitle\": post.postTitle,\n",
    "            \"postExcerpt\": post.postExcerpt,\n",
    "            \"postContent\": post.postContent,\n",
    "            \"postDate\": post.postDate,\n",
    "            \"postAuthor\": post.postAuthor,\n",
    "            \"postCategories\": post.postCategories,\n",
    "            \"postTags\": post.postTags,\n",
    "            \"postUrl\": post.postUrl,\n",
    "            \"postSequence\": post.postSequence,\n",
    "            \"isDeleted\": post.isDeleted\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_dict(data:dict):\n",
    "        if not data or type(data) is not dict:\n",
    "            return None\n",
    "\n",
    "        return Post(\n",
    "            id=data.get(\"id\", \"\"),\n",
    "            postId=data.get(\"postId\", \"\"),\n",
    "            postTitle=data.get(\"postTitle\", \"\"),\n",
    "            postExcerpt=data.get(\"postExcerpt\", \"\"),\n",
    "            postContent=data.get(\"postContent\", \"\"),\n",
    "            postDate=data.get(\"postDate\", \"\"),\n",
    "            postAuthor=data.get(\"postAuthor\", \"\"),\n",
    "            postCategories=data.get(\"postCategories\", \"\"),\n",
    "            postTags=data.get(\"postTags\", \"\"),\n",
    "            postUrl=data.get(\"postUrl\", \"\"),\n",
    "            postSequence=data.get(\"postSequence\", 1),\n",
    "            isDeleted=data.get(\"isDeleted\", False)\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_field_mapping()->dict:\n",
    "        return {\n",
    "            'postId': 'id',\n",
    "            'postTitle': 'title',\n",
    "            'postExcerpt': 'excerpt',\n",
    "            'postContent': 'content',\n",
    "            'postDate': 'date_gmt',\n",
    "            'postAuthor': 'author',\n",
    "            'postCategories': 'categories',\n",
    "            'postTags': 'tags',\n",
    "            'postUrl': 'link',\n",
    "            'postSequence': 'sequence',\n",
    "            'isDeleted': 'isDeleted'\n",
    "        }\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aec29-cd61-4ea2-874c-4c8140d018f0",
   "metadata": {},
   "source": [
    "## 4.0 LLM Configuration\n",
    "\n",
    "**Model: Gemma-2-2b**\n",
    "\n",
    "**Source: [https://huggingface.co/google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b)**\n",
    "\n",
    "**Source: [Fine-tuning Llama2](https://github.dev/krishnaik06/Finetuning-LLM/blob/main/Fine_tune_Llama_2.ipynb)**\n",
    "\n",
    "**Quantization: 4bit**\n",
    "\n",
    "**Tokenizer: Gemma-2-2b**\n",
    "\n",
    "**Technique: [Lora](https://huggingface.co/docs/peft/package_reference/lora)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622b24c-f1ef-4a7c-b39b-488c822a4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate bitsandbytes peft scikit-learn scipy trl transformers tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4ddf3-8b4b-4044-9240-20a26ef4619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuning use pytorch with cuda. For backend application, use the one without cuda to reduce unnceccessary package size\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c1057a-98a5-4cc5-8271-93540fa281c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f362e379-4e36-49d1-872f-feb8e5d41251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\accelerate\\utils\\other.py:220: DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
      "  np.core.multiarray._reconstruct,\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0797a61-35da-4519-b03e-8d5535cd3086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 2060 SUPER\n",
      "Memory Allocated: 0 bytes\n",
      "Memory Cached: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_details():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "        print(f\"Memory Cached: {torch.cuda.memory_reserved()} bytes\")\n",
    "\n",
    "get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6519ff6d-e811-4c11-9e7c-d2dd3e2d7e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Clearing GPU memory...\")\n",
    "        torch.cuda.empty_cache()  # Clears the GPU cache\n",
    "        torch.cuda.reset_peak_memory_stats()  # Resets memory stats tracking\n",
    "        torch.cuda.synchronize()  # Ensures all streams are synced (optional)\n",
    "        print(\"GPU memory cleared.\")\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(\"CUDA is not available. No GPU memory to clear.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83940a04-e6af-4f2a-9ca3-14fb4d764c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and tokenizer\n",
    "model_name = \"google/gemma-2b-it\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Create a sample input\n",
    "text = \"The GPU is being tested with transformers!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Perform a forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Verify that the tensors are on the GPU\n",
    "if torch.cuda.is_available():\n",
    "    is_on_gpu = all(tensor.device.type == 'cuda' for tensor in outputs.values())\n",
    "    if is_on_gpu:\n",
    "        print(\"Success: Transformers library is using the GPU.\")\n",
    "    else:\n",
    "        print(\"Warning: Transformers library is not using the GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n",
    "\n",
    "# Optional: Print GPU details\n",
    "get_gpu_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49550409-5ba8-478a-924a-0fe68591ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing GPU memory...\n",
      "GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a121cb15-975a-4b3a-a0b4-6c5fc4d3c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"google/gemma-2b-it\" #\"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = f\"{model_name}-chat-finetune\" #\"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7b25eca-3c69-4a52-8084-3864b644d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b83c2d7-efe3-4347-8786-63a8b3fd6818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load base model \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        token=hf_token,\n",
    "    )\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d183fa6-eb47-4234-a8cb-e68bc80df1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemma tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691316f9-7673-4d55-a774-97806d06be2c",
   "metadata": {},
   "source": [
    "## 5.0 Predict output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0459e3e5-558a-49ab-b465-a6d1bb1464e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[    2,  1841,   603,   573,  5467,   576,   777,  4521,  3855, 26863]],\n",
      "       device='cuda:0')\n",
      "{'generated_text': \"What is the origin of 'Hello World!'?\\n\\nThe origin of 'Hello World!' is a computer program written by John A. Wheeler in 1969. The program was written as a demonstration of the capabilities of the IBM 1401, a mainframe computer.\\n\\nWheeler wrote the program as a way to show off his programming skills and to demonstrate the power of the IBM 1401. The program was executed on the IBM 1401, and it produced the following output:\\n\\n```\\nHello, world!\\n```\\n\\nWheeler later ported the program to other computers, including the Apple II and the Commodore 64. The program became a popular demonstration of the power of computers, and it was used in many educational settings.\\n\\nToday, 'Hello World!' is one of the most famous and recognizable computer programs in the world. It is a reminder of the power of computers and the importance of learning to code.\"}\n"
     ]
    }
   ],
   "source": [
    "# Generate output using pipeline and predefined prompts by HuggingFace\n",
    "\n",
    "text = \"What is the origin of 'Hello World!'\"\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "print(type(encoded_text))\n",
    "print(tokenizer.encode(text, return_tensors='pt').to(\"cuda\"))\n",
    "pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(text)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31251c32-26ba-41da-b92b-90e8520fed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "What is the origin of 'Hello World!'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "<bos><start_of_turn>user\n",
      "What is the origin of 'Hello World!'<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The origin of 'Hello World!' is traced back to the early days of computer programming in the 1940s.\n",
      "\n",
      "**1. The Birth of Computer Programming:**\n",
      "- The first known use of the term \"Hello World!\" was in a 1947 paper by John Atanasoff and Clifford Berry at the University of California, Berkeley.\n",
      "- They wrote a program that printed a message to the console, \"Hello, world!\"\n",
      "\n",
      "**2. The First Compiler:**\n",
      "- Atanasoff and Berry's program used a compiler called \"COBOL\" (COmputer Programming Language).\n",
      "- COBOL was a high-level compiler that translated human-readable code into machine-readable instructions.\n",
      "\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "# Generate output by constructing prompt from raw text query\n",
    "text = \"What is the origin of 'Hello World!'\"\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": text },\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7f81a-458b-46a7-b839-34dc74616b4c",
   "metadata": {},
   "source": [
    "## 6.0 Tokenizing Posts\n",
    "\n",
    "We will generate embeddings externally using Gemma's tokenizer and store it in weaviate. This will enable not only to perform semantic search in the databaset, it will also keep the vectors prepared and stored reducing the time to generate vectors each time a request is made.\n",
    "\n",
    "1. The following parameters will be included in the embedding:\n",
    "\n",
    "    - postContent: Main content of the post\n",
    "    - postTitle: Title of the post\n",
    "    - postAuthor: Author of the post\n",
    "    - postDate: Publish date of the post\n",
    "    - postCategories: Categories that the post belongs to\n",
    "    - postTags: Tags related to the post\n",
    "\n",
    "    \n",
    "\n",
    "2. The following type of embeddings will be available for the system.\n",
    "\n",
    "    - **Fine-tuning:** When fine-tuning the model, the embedding will contain post contents with some metadata. The text needs to be conversational.\n",
    "    - **Query:** When querying the LLM, the text that will be converted to embedding will be different from the ones that are used for fine-tuning. The text will contain instructions, user prompt, and a context. The combined text will form the embedding.\n",
    "\n",
    "### 6.1 Dataset for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d7c7f9c-b9eb-4405-b244-13f21b2192d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_collection = async_client.collections.get(\"Post\")\n",
    "posts = await post_collection.query.fetch_objects(limit=3)\n",
    "\n",
    "def post_to_embedding_content(post):\n",
    "    embedding_content = \"\"\n",
    "    embedding_content += f\"Post Title: {post.properties['postTitle']}\\n\"\n",
    "    embedding_content += f\"Post Author: {post.properties['postAuthor']}\\n\"\n",
    "    embedding_content += f\"Post Date: {post.properties['postDate']}\\n\"\n",
    "    embedding_content += f\"Post Categories: {post.properties['postCategories']}\\n\"\n",
    "    embedding_content += f\"Post Tags: {post.properties['postTags']}\\n\"\n",
    "    embedding_content += f\"Post Content: {post.properties['postContent']}\"\n",
    "    return embedding_content\n",
    "# print(post_to_embedding_content(posts.objects[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb65df7-9dd4-495d-abf2-e4fab672315f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for post in posts.objects:\n",
    "    dataset.append({\n",
    "        \"text\": post_to_embedding_content(post)\n",
    "    })\n",
    "dataset = Dataset.from_list(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e56cf37e-789c-4112-ab7f-983cd9c93aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await post_collection.length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d6a09c7-0aa2-49c5-ae63-ba9c0bea4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def save_dataset_as_json():\n",
    "    posts = await post_collection.query.fetch_objects()\n",
    "    dataset = []\n",
    "    for post in posts.objects:\n",
    "        dataset.append({\n",
    "            \"text\": post_to_embedding_content(post)\n",
    "        })\n",
    "    with open(\"dataset.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(dataset, f)\n",
    "\n",
    "# await save_dataset_as_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f34449-e974-4249-b12b-a87f45963368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 333.50 examples/s]\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\executing\\executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "E:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.37 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 235.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 45\u001b[0m\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     packing\u001b[38;5;241m=\u001b[39mpacking,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2125\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2126\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2128\u001b[0m     )\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\peft\\peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1645\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1646\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1647\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1648\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1649\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1650\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1651\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1652\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1653\u001b[0m         )\n\u001b[0;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\accelerate\\hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[1;34m(module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:1091\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1089\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1091\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function(logits, labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1094\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mE:\\Accessories\\WebDevelopment\\Portfolio\\portfolio_admin_experience_django\\llm\\venv\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:38\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[1;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     39\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.37 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 235.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Needed for google/gemma-2b-it\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1af55f-0588-41e4-9204-c4f9c21ee7b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7.0 Prepare dataset for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f81f35-02ee-4a96-873a-00a39a308f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
